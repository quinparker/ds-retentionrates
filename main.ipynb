{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention rates for US Universities\n",
    "\n",
    "This project is being built from the ground up to be customizable and reproducible.\n",
    "\n",
    "After importing the libraries we need, we're going to load up a table of instructions containing which\n",
    "of the several thousand attributes we're going to select from the IPEDS database for data mining. This is a lot, so it seems more reasonable to externally store the data list we'll be drawing from.\n",
    "\n",
    "Many of these, such as website addresses or mission statements, aren't going to be terribly useful. Some others we'll need to exclude as it is too closely related to retention rate, and we risk overfitting or circular logic (\"hey, here's how to raise your retention rate--have more of them graduate!\")\n",
    "\n",
    "At the root, each entry in the JSON file denotes a separate table. Included alongside the table name are instructions on whether all the table should be imported as default or not, which attributes are continuous, which are discrete, and  which are strings, and whether multiple records exist for each primary key. This is all derived from the associated documentation that comes with IPEDS. \n",
    "\n",
    "We're going to load in each table (obviously checking that it doesn't exist first), and extract the correct tables from it. \n",
    "\n",
    "(Should you want to change what we're measuring, you can change the response variable within the JSON file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import scipy\n",
    "import json\n",
    "import wget\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from zipfile import ZipFile as zf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we're going to set some variables for how (and where) we process the data.\n",
    "\n",
    "* *MAXIMUM_NAN* : the ratio of NaNs at which we remove the column entirely.\n",
    "* *MAXIMUM_COR* : the maximum Pearson correlation that a column can have with another before it's removed.\n",
    "* *SAMPLE_SIZE* : the fraction of the final dataset to use to build the tree\n",
    "* *DATA_PATH* : where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXIMUM_NAN = 0.80\n",
    "MAXIMUM_COR = 0.85\n",
    "SAMPLE_SIZE = 0.90\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading in and cleaning the IPEDS and ACS data\n",
    "\n",
    "Next come the functions for reading in the data from files/ZIP archives, corraling them all to one row per unique key, and making sure they're the right data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported HD2016: 27 columns total, 0.4MB\n",
      "Imported IC2016: 132 columns total, 1.67MB\n",
      "Imported IC2016_AY: 252 columns total, 8.56MB\n",
      "Imported ADM2016: 290 columns total, 10.74MB\n",
      "Imported EFFY2016: 380 columns total, 15.91MB\n",
      "Imported EF2016A: 650 columns total, 31.4MB\n",
      "Imported EF2016B: 776 columns total, 38.63MB\n",
      "Imported EF2016C: 906 columns total, 46.09MB\n",
      "Imported EF2016D: 917 columns total, 46.72MB\n",
      "Imported EF2016A_DIST: 962 columns total, 49.3MB\n",
      "Imported SFA1516: 1285 columns total, 67.83MB\n",
      "Imported SFAV1516: 1303 columns total, 68.87MB\n",
      "Imported F1516_F1A: 1413 columns total, 75.13MB\n",
      "Imported F1516_F2: 1553 columns total, 83.11MB\n",
      "Imported F1516_F3: 1632 columns total, 87.53MB\n",
      "Imported EAP2016: 3990 columns total, 222.84MB\n",
      "Imported SAL2016_IS: 4123 columns total, 230.47MB\n",
      "Imported SAL2016_NIS: 4151 columns total, 232.08MB\n",
      "Imported S2016_OC: 4833 columns total, 271.21MB\n",
      "Imported S2016_SIS: 4910 columns total, 275.63MB\n",
      "Imported S2016_NH: 4941 columns total, 277.41MB\n",
      "Imported AL2016: 4971 columns total, 279.13MB\n"
     ]
    }
   ],
   "source": [
    "def convert(tdf, columnlist, totype):\n",
    "\n",
    "# this changes the datatype in each column. where columns have been unstacked, this is supposed\n",
    "# to change all instances of each unstacked column to that datatype\n",
    "\n",
    "    for col in columnlist:\n",
    "        r = re.compile(\"(^\"+col+\"*)|(_\"+col+\")\")\n",
    "        # column name must either be first or have a _ before it, depending on whether we're\n",
    "        # putting the table names next to it to make it unique\n",
    "        for c in filter(r.match, list(tdf)): # filter all the column names through this regex\n",
    "            tdf[c] = tdf[c].astype(totype)\n",
    "\n",
    "def import_data(filename):\n",
    "    with open(filename) as file:\n",
    "        instructions = json.load(file)\n",
    "        \n",
    "# does the data directory exist? if no, create it.\n",
    "    \n",
    "    if os.path.isdir(DATA_PATH) is False:\n",
    "        try:  \n",
    "            os.mkdir(DATA_PATH)\n",
    "        except OSError:  \n",
    "            print (\"Could not create data folder.\")\n",
    "            raise\n",
    "            \n",
    "# start with an empty dataframe to fill, and get the primary key.\n",
    "# the primary key should be in the first file in the data list\n",
    "# also figure out if we are adding the tablename to the attributes to make them unique\n",
    "\n",
    "    ourdata = pd.DataFrame()\n",
    "    pk = instructions[\"primarykey\"]\n",
    "    unique_headers = instructions[\"uniqueheaders\"]\n",
    "\n",
    "# loop through each table. \n",
    "\n",
    "    for table in instructions[\"tables\"]:\n",
    "        \n",
    "# first, check if the file in the table has been downloaded. if not, download it from\n",
    "# the path given and throw an error if something is wrong.\n",
    "\n",
    "        filename = table[\"name\"]+instructions[\"format\"]\n",
    "        filelocation = DATA_PATH+\"/\"+filename\n",
    "        csvfile = DATA_PATH+\"/\"+table[\"name\"]+\".csv\"\n",
    "\n",
    "        if instructions[\"lowercase\"]:\n",
    "            csvfile = str(csvfile).lower()\n",
    "\n",
    "        if os.path.exists(filelocation) is False:\n",
    "            try:\n",
    "                wget.download(instructions[\"url\"]+filename, filelocation)\n",
    "            except Exception as e:\n",
    "                print(\"Problem downloading and saving\", table[\"name\"], \":\", e)\n",
    "                raise\n",
    "\n",
    "# next, if they are zip files, unzip them\n",
    "\n",
    "        if instructions[\"format\"] == \".zip\":\n",
    "            if os.path.exists(csvfile) is False:\n",
    "                print(\"Unzipping \"+table[\"name\"])\n",
    "                with zf(filelocation,\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "# load each CSV file into a temporary data frame, tdf\n",
    "\n",
    "        if \"skiprows\" in instructions:\n",
    "            tdf = pd.read_csv(str(csvfile), encoding = \"ISO-8859-1\",\n",
    "                              skiprows = lambda x: x in instructions[\"skiprows\"])\n",
    "        else:\n",
    "            tdf = pd.read_csv(str(csvfile), encoding = \"ISO-8859-1\")\n",
    "\n",
    "# filter the data according to any values needed\n",
    "\n",
    "        if \"filter\" in table:\n",
    "            for filterinfo in table[\"filter\"]:\n",
    "                tdf = tdf.loc[tdf[filterinfo[0]] == filterinfo[1]]\n",
    "\n",
    "# then, depending on the instructions in the JSON file, include all the headers, or include a selection.\n",
    "\n",
    "        if table[\"includeall\"]:\n",
    "\n",
    "# include the whole list, excluding the specific tables (might be an empty list)\n",
    "# and add the primary key to select\n",
    "\n",
    "            to_include = list(tdf)\n",
    "            to_exclude = table[\"exclude\"]\n",
    "            headers = [x for x in to_include if x not in to_exclude]\n",
    "            headers.append(pk)\n",
    "\n",
    "# otherwise, stick these three lists together plus the primary key\n",
    "\n",
    "        else:\n",
    "            headers = [pk, *table[\"strings\"], *table[\"discrete\"], *table[\"continuous\"]]\n",
    "\n",
    "        selected_headers = [x for x in tdf.columns if x in headers]\n",
    "\n",
    "# columns that begin with an X should be removed, at least for now, because they don't describe\n",
    "# anything other than how the data was collected\n",
    "# we should also remove any Margin of Error data (_MOE_), it's not needed\n",
    "\n",
    "        selected_headers = [x for x in selected_headers if not re.search(\"(?:_MOE_)|(?:^X\\s*)\", x)]\n",
    "\n",
    "        # ok, so do we have a primary key? if not, stop right there\n",
    "\n",
    "        if pk not in tdf.columns:\n",
    "            raise KeyError(\"Primary key \"+pk+\" not found in \"+table[\"name\"])\n",
    "\n",
    "# now we can select the headers we want.\n",
    "\n",
    "        tdf = tdf[selected_headers]\n",
    "    \n",
    "# the code below adds the table name to the headers now, to prevent issues with duplication\n",
    "# to everythng other than the key\n",
    "\n",
    "        if unique_headers:\n",
    "            tdf.rename(columns = lambda x: pk if x == pk else table[\"name\"]+\"_\"+x, inplace = True)\n",
    "\n",
    "# next we must check in the JSON instructions if this table contains multiple rows for each\n",
    "# unique ID. if so, we need to put them all on the same row. to do this, we change them to strings\n",
    "# then read them into a multiple index, unstack, and then join the column names.\n",
    "\n",
    "        if \"multi\" in table:\n",
    "\n",
    "            if unique_headers:\n",
    "                multi = [table[\"name\"]+\"_\"+x for x in table[\"multi\"]]\n",
    "            else:\n",
    "                multi = table[\"multi\"]\n",
    "            tdf[multi] = tdf[multi].astype(str)\n",
    "            tdf = tdf.set_index([pk, *multi])\n",
    "            tdf = tdf.unstack(multi)               # need to specify ALL the levels\n",
    "            tdf.columns = ['_'.join(col) for col in tdf.columns.values]\n",
    "            tdf = tdf.reset_index(level=pk)\n",
    "\n",
    "# any '.', '. ', '(X)', '****'', '-' data should be NaN\n",
    "\n",
    "        tdf = tdf.replace(r\"(^\\. ?$)|(^\\(X\\)$)|(^N$)|(^\\*{2,}$)|(^-$)\", np.nan, regex=True)\n",
    "\n",
    "# also, apparently some of the ACS data has '-' and '+' signs at the end of some estimates.\n",
    "# this is not too helpful if we're casting to float, so we'll remove them.\n",
    "\n",
    "        tdf = tdf.replace(r\"(\\+$)|(\\-$)\", \"\", regex=True)\n",
    "\n",
    "# and, for some reason, actual commas in numerals. replace them in numbers only, though.\n",
    "\n",
    "        tdf = tdf.replace(r\"(\\d+)(,)(\\d+)\", r\"\\1\\3\", regex=True)\n",
    "\n",
    "# one important thing to do here is to set the right data types--strings or discrete data.\n",
    "# most of the data in IPEDS and ACS are continuous, but there are a couple of strings and a \n",
    "# handful of discrete data.\n",
    "\n",
    "        if \"defaulttype\" in table:\n",
    "            tdfheds = list(tdf) # get list of headers\n",
    "            hedstoremove = [pk, *table[\"strings\"], *table[\"discrete\"], *table[\"continuous\"]]\n",
    "            tdfheds = [x for x in tdfheds if x not in hedstoremove]\n",
    "            if table[\"defaulttype\"] == \"discrete\":\n",
    "                convert(tdf, tdfheds, \"category\")  \n",
    "            elif table[\"defaulttype\"] == \"string\":\n",
    "                convert(tdf, tdfheds, \"str\")\n",
    "            else:\n",
    "                convert(tdf, tdfheds, \"float\")\n",
    "                # only way to store NaNs\n",
    "\n",
    "        convert(tdf, table[\"strings\"], \"str\")\n",
    "        convert(tdf, table[\"discrete\"], \"category\")\n",
    "        convert(tdf, table[\"continuous\"], \"float\")\n",
    "        \n",
    "        if pk in tdf:\n",
    "            tdf[pk] = tdf[pk].astype('str') # it is in the index if the data is unstacked\n",
    "\n",
    "# if it's the first time around the loop, take the first set of data.\n",
    "\n",
    "        if ourdata.empty:\n",
    "            ourdata = tdf\n",
    "\n",
    "# if not, then we need to join on the primary key\n",
    "\n",
    "        else:\n",
    "            ourdata = ourdata.merge(tdf,on=pk,how=\"left\")\n",
    "\n",
    "        print(\"Imported \"+ table[\"name\"]+ \": \"+str(len(ourdata.columns))+\" columns total, \"\n",
    "              + str(round(float(ourdata.memory_usage().sum() / 1048576), 2)) + \"MB\")\n",
    "    \n",
    "    if \"responsevar\" in instructions:\n",
    "        return ourdata, instructions[\"responsevar\"]\n",
    "    else:\n",
    "        return ourdata, None\n",
    "\n",
    "ipeds, responsevar = import_data(\"ipeds-instructions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the ACS data. This is 2016 1-year data, put together on a county level by the\n",
    "American Fact Finder. Unfortunately, this data had to be generated and wasn't available for download.\n",
    "In order to make this reproducible, I've uploaded the generated files to GitHub.\n",
    "\n",
    "This data contains info, county-by-county, on employment, healthcare, commuting, income, and even things like how many computers each household has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported ACS_16_1YR_S0101_with_ann: 109 columns total, 0.69MB\n",
      "Imported ACS_16_1YR_S0701_with_ann: 389 columns total, 2.47MB\n",
      "Imported ACS_16_1YR_S0801_with_ann: 560 columns total, 3.56MB\n",
      "Imported ACS_16_1YR_S1101_with_ann: 660 columns total, 4.19MB\n",
      "Imported ACS_16_1YR_S1701_with_ann: 843 columns total, 5.35MB\n",
      "Imported ACS_16_1YR_S1810_with_ann: 1050 columns total, 6.66MB\n",
      "Imported ACS_16_1YR_S2201_with_ann: 1278 columns total, 8.11MB\n",
      "Imported ACS_16_1YR_S2301_with_ann: 1418 columns total, 9.0MB\n",
      "Imported ACS_16_1YR_S2405_with_ann: 1508 columns total, 9.57MB\n",
      "Imported ACS_16_1YR_S2701_with_ann: 1818 columns total, 11.53MB\n",
      "Imported ACS_16_1YR_S2702_with_ann: 2028 columns total, 12.86MB\n",
      "Imported ACS_16_1YR_S2801_with_ann: 2090 columns total, 13.26MB\n",
      "Imported ACS_16_1YR_S2802_with_ann: 2244 columns total, 14.23MB\n",
      "Imported ACS_16_1YR_B01003_with_ann: 2246 columns total, 14.25MB\n",
      "Imported ACS_16_1YR_B05003_with_ann: 2292 columns total, 14.54MB\n",
      "Imported ACS_16_1YR_B08301_with_ann: 2334 columns total, 14.8MB\n",
      "Imported ACS_16_1YR_B08302_with_ann: 2364 columns total, 14.99MB\n",
      "Imported ACS_16_1YR_B15003_with_ann: 2414 columns total, 15.31MB\n",
      "Imported ACS_16_1YR_B19051_with_ann: 2420 columns total, 15.35MB\n",
      "Imported ACS_16_1YR_B19055_with_ann: 2426 columns total, 15.39MB\n",
      "Imported ACS_16_1YR_B19057_with_ann: 2432 columns total, 15.43MB\n",
      "Imported ACS_16_1YR_B23013_with_ann: 2438 columns total, 15.46MB\n",
      "Imported ACS_16_1YR_B23020_with_ann: 2444 columns total, 15.5MB\n",
      "Imported ACS_16_1YR_B23025_with_ann: 2458 columns total, 15.59MB\n",
      "Imported ACS_16_1YR_B25001_with_ann: 2460 columns total, 15.6MB\n",
      "Imported ACS_16_1YR_B25002_with_ann: 2466 columns total, 15.64MB\n",
      "Imported ACS_16_1YR_B25003_with_ann: 2472 columns total, 15.68MB\n",
      "Imported ACS_16_1YR_B25031_with_ann: 2486 columns total, 15.77MB\n",
      "Imported ACS_16_1YR_B25058_with_ann: 2488 columns total, 15.78MB\n",
      "Imported ACS_16_1YR_B25094_with_ann: 2524 columns total, 16.01MB\n",
      "Imported ACS_16_1YR_B28010_with_ann: 2538 columns total, 16.1MB\n",
      "Imported ACS_16_1YR_B28011_with_ann: 2554 columns total, 16.2MB\n"
     ]
    }
   ],
   "source": [
    "acs, x = import_data(\"acs-instructions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the two-letter state codes and the county names. We now need to convert them to County Name, State in order to make the join with the American Community Survey data. We'll create a new column and drop the old county name column.\n",
    "\n",
    "As you can see from the selection below, there's a one-to-many relationship between the counties and the educational institutions. And, of course, many counties won't have institutions in at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000       Dupage County, Illinois\n",
       "1001         Cook County, Illinois\n",
       "1002         Cook County, Illinois\n",
       "1003         Cook County, Illinois\n",
       "1004         Cook County, Illinois\n",
       "1005         Cook County, Illinois\n",
       "1006         Cook County, Illinois\n",
       "1007         Cook County, Illinois\n",
       "1008         Cook County, Illinois\n",
       "1009         Cook County, Illinois\n",
       "1010         Cook County, Illinois\n",
       "1011         Cook County, Illinois\n",
       "1012         Cook County, Illinois\n",
       "1013         Cook County, Illinois\n",
       "1014         Cook County, Illinois\n",
       "1015         Cook County, Illinois\n",
       "1016         Cook County, Illinois\n",
       "1017    Vermilion County, Illinois\n",
       "1018    Vermilion County, Illinois\n",
       "1019         Cook County, Illinois\n",
       "1020       Dupage County, Illinois\n",
       "1021      Mchenry County, Illinois\n",
       "1022       Dupage County, Illinois\n",
       "1023         Cook County, Illinois\n",
       "1024        Coles County, Illinois\n",
       "Name: County Name, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"states_hash.json\") as file:\n",
    "    states = json.load(file)\n",
    "if \"County Name\" not in ipeds:\n",
    "    ipeds[\"County Name\"] = ipeds[\"COUNTYNM\"]+\", \"+ipeds[\"STABBR\"].map(states)\n",
    "    ipeds = ipeds.drop(columns=\"COUNTYNM\")\n",
    "ipeds[\"County Name\"][1000:1025]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join them and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNITID</th>\n",
       "      <th>INSTNM</th>\n",
       "      <th>STABBR</th>\n",
       "      <th>OBEREG</th>\n",
       "      <th>OPEFLAG</th>\n",
       "      <th>ICLEVEL</th>\n",
       "      <th>CONTROL</th>\n",
       "      <th>HLOFFER</th>\n",
       "      <th>UGOFFER</th>\n",
       "      <th>GROFFER</th>\n",
       "      <th>...</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD01_VD04</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD02_VD04</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD01_VD05</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD02_VD05</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD01_VD06</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD02_VD06</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD01_VD07</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD02_VD07</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD01_VD08</th>\n",
       "      <th>ACS_16_1YR_B28011_with_ann_HD02_VD08</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100654</td>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>AL</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>102602.0</td>\n",
       "      <td>3229.0</td>\n",
       "      <td>8040.0</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>3293.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>16880.0</td>\n",
       "      <td>2212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100663</td>\n",
       "      <td>University of Alabama at Birmingham</td>\n",
       "      <td>AL</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>169188.0</td>\n",
       "      <td>4550.0</td>\n",
       "      <td>15666.0</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>6488.0</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>8299.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>47189.0</td>\n",
       "      <td>2665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100690</td>\n",
       "      <td>Amridge University</td>\n",
       "      <td>AL</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>59659.0</td>\n",
       "      <td>2858.0</td>\n",
       "      <td>2608.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>16752.0</td>\n",
       "      <td>1601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100706</td>\n",
       "      <td>University of Alabama in Huntsville</td>\n",
       "      <td>AL</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>102602.0</td>\n",
       "      <td>3229.0</td>\n",
       "      <td>8040.0</td>\n",
       "      <td>1530.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>3293.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>16880.0</td>\n",
       "      <td>2212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100724</td>\n",
       "      <td>Alabama State University</td>\n",
       "      <td>AL</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>59659.0</td>\n",
       "      <td>2858.0</td>\n",
       "      <td>2608.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>16752.0</td>\n",
       "      <td>1601.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7524 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UNITID                               INSTNM STABBR OBEREG OPEFLAG ICLEVEL  \\\n",
       "0  100654             Alabama A & M University     AL      5       1       1   \n",
       "1  100663  University of Alabama at Birmingham     AL      5       1       1   \n",
       "2  100690                   Amridge University     AL      5       1       1   \n",
       "3  100706  University of Alabama in Huntsville     AL      5       1       1   \n",
       "4  100724             Alabama State University     AL      5       1       1   \n",
       "\n",
       "  CONTROL HLOFFER UGOFFER GROFFER                 ...                   \\\n",
       "0       1       9       1       1                 ...                    \n",
       "1       1       9       1       1                 ...                    \n",
       "2       2       9       1       1                 ...                    \n",
       "3       1       9       1       1                 ...                    \n",
       "4       1       9       1       1                 ...                    \n",
       "\n",
       "  ACS_16_1YR_B28011_with_ann_HD01_VD04 ACS_16_1YR_B28011_with_ann_HD02_VD04  \\\n",
       "0                             102602.0                               3229.0   \n",
       "1                             169188.0                               4550.0   \n",
       "2                              59659.0                               2858.0   \n",
       "3                             102602.0                               3229.0   \n",
       "4                              59659.0                               2858.0   \n",
       "\n",
       "  ACS_16_1YR_B28011_with_ann_HD01_VD05 ACS_16_1YR_B28011_with_ann_HD02_VD05  \\\n",
       "0                               8040.0                               1530.0   \n",
       "1                              15666.0                               1648.0   \n",
       "2                               2608.0                                614.0   \n",
       "3                               8040.0                               1530.0   \n",
       "4                               2608.0                                614.0   \n",
       "\n",
       "  ACS_16_1YR_B28011_with_ann_HD01_VD06 ACS_16_1YR_B28011_with_ann_HD02_VD06  \\\n",
       "0                               1146.0                                546.0   \n",
       "1                               6488.0                               1442.0   \n",
       "2                                494.0                                373.0   \n",
       "3                               1146.0                                546.0   \n",
       "4                                494.0                                373.0   \n",
       "\n",
       "  ACS_16_1YR_B28011_with_ann_HD01_VD07 ACS_16_1YR_B28011_with_ann_HD02_VD07  \\\n",
       "0                               3293.0                                851.0   \n",
       "1                               8299.0                               1301.0   \n",
       "2                               1382.0                                573.0   \n",
       "3                               3293.0                                851.0   \n",
       "4                               1382.0                                573.0   \n",
       "\n",
       "  ACS_16_1YR_B28011_with_ann_HD01_VD08 ACS_16_1YR_B28011_with_ann_HD02_VD08  \n",
       "0                              16880.0                               2212.0  \n",
       "1                              47189.0                               2665.0  \n",
       "2                              16752.0                               1601.0  \n",
       "3                              16880.0                               2212.0  \n",
       "4                              16752.0                               1601.0  \n",
       "\n",
       "[5 rows x 7524 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs = acs.rename(columns={\"GEO.display-label\": \"County Name\"})\n",
    "ipeds = ipeds.merge(acs,on=\"County Name\",how=\"left\")\n",
    "ipeds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Preparing the data for analysis\n",
    "\n",
    "This is, clearly, a horrfying amount of data. Let's remove surplus columns before putting them under analysis. First, find out how many NaNs there are for each column and remove those above the threshold. (We should do this before figuring out correlations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.79% of columns removed for missing too much data\n"
     ]
    }
   ],
   "source": [
    "original_length = len(ipeds.columns)\n",
    "\n",
    "nanlist = ipeds.isna().sum(axis=0) / len(ipeds)\n",
    "nans = nanlist.loc[nanlist >= MAXIMUM_NAN]\n",
    "\n",
    "print (str(round(float(len(nans) / len(ipeds)),4) * 100) + \n",
    "       \"% of columns removed for missing too much data\")\n",
    "\n",
    "ipeds = ipeds.drop(columns = nans.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too much point trying to build a tree around the response variable if it doesn't exist.\n",
    "Remove the rows where it's missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norv = ipeds[responsevar].isna()\n",
    "ipeds = ipeds.drop(ipeds[norv].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to find out which datasets are too similar to one another, and remove them. \n",
    "The most basic way to do this is to do a Pearson correlation between all of the attributes, and\n",
    "then remove one of each pair, or all-but-one of each collection.\n",
    "\n",
    "As there are around 5,000 attributes by this point, this will take a little while to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROOMCAP</td>\n",
       "      <td>ENRLT</td>\n",
       "      <td>0.867458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROOMCAP</td>\n",
       "      <td>ENRLM</td>\n",
       "      <td>0.868164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ROOMCAP</td>\n",
       "      <td>ENRLFT</td>\n",
       "      <td>0.874227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ROOMCAP</td>\n",
       "      <td>ENRLFTM</td>\n",
       "      <td>0.874670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ROOMCAP</td>\n",
       "      <td>ENRLFTW</td>\n",
       "      <td>0.850474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A        B  Correlation\n",
       "97   ROOMCAP    ENRLT     0.867458\n",
       "98   ROOMCAP    ENRLM     0.868164\n",
       "100  ROOMCAP   ENRLFT     0.874227\n",
       "101  ROOMCAP  ENRLFTM     0.874670\n",
       "102  ROOMCAP  ENRLFTW     0.850474"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the pearson correlation between each column. \n",
    "\n",
    "cmatrix = ipeds.corr(method=\"pearson\").abs()\n",
    "\n",
    "# change the matrix to a dataframe, and put the indices in the first two columns\n",
    "\n",
    "c = cmatrix.unstack().to_frame().reset_index()\n",
    "c.columns = [\"A\", \"B\", \"Correlation\"]\n",
    "\n",
    "# remove NaNs\n",
    "c = c.dropna()\n",
    "\n",
    "# remove pairs (everything correlates with itself)\n",
    "c = c.loc[c[\"A\"] != c[\"B\"]]\n",
    "\n",
    "# and remove everything that is less than the maximum correlation.\n",
    "c = c.loc[c[\"Correlation\"] >= MAXIMUM_COR]\n",
    "\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of attributes: 7524\n",
      "Reduced number of attributes: 1177\n",
      "15.64% of original.\n"
     ]
    }
   ],
   "source": [
    "# we now have a list of pairs of variables that correlate with each other.\n",
    "# now we need to extract which variables to remove from our main dataframe.\n",
    "\n",
    "# first, let's sort the two columns into alphabetical order so that the first always lies behind\n",
    "# the second. we do this by applying a lambda across each row, swapping them if they're not in\n",
    "# order, and broadcasting the result back to the dataframe. \n",
    "\n",
    "c = c.apply(lambda row: [row[\"B\"], row[\"A\"], row[\"Correlation\"]] \n",
    "                         if row[\"A\"] > row[\"B\"] else [row[\"A\"], row[\"B\"], row[\"Correlation\"]],\n",
    "                         axis=1, result_type='broadcast')\n",
    "\n",
    "# we then drop all the duplicates. this should remove probably half or so of the rows.\n",
    "\n",
    "c = c.drop_duplicates()\n",
    "\n",
    "# if we then just take the first column, and dedupe that, we'll get a list of columns we can remove.\n",
    "# if there are more than 2 columns that correlate with one another, one column will always\n",
    "# only appear in the 'B' list. this will be the one that goes forward to the next stage.\n",
    "\n",
    "cols_to_remove = c[\"A\"].drop_duplicates()\n",
    "\n",
    "ipeds = ipeds.drop(columns = cols_to_remove)\n",
    "\n",
    "print (\"Original number of attributes: \"+str(original_length))\n",
    "print (\"Reduced number of attributes: \"+str(len(ipeds.columns)))\n",
    "print (str(round(float(len(ipeds.columns) / original_length),4) * 100) + \"% of original.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a decision tree\n",
    "\n",
    "The response variable, PCT_RCF, is continuous -- a percentage from 0-100%. It's most appropriate, therefore, to build a decision tree regressor in order to find out which attributes are most likely to affect the retention rate.\n",
    "\n",
    "We could decide to classify above or below the mean, but a retention rate is highly contextual based on the college, the environment, and so on -- so there may be different factors in play for higher retention than for lower retention.\n",
    "\n",
    "Starting with the whole dataset, we'll build training and test sets, set a random seed, and then build a tree out.\n",
    "\n",
    "But first, we need to fill in the rest of those NaNs. I'm using most common for categorical, and mean for continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "regressortree = RandomForestRegressor(random_state=55)\n",
    "\n",
    "# we have removed a bunch of data and our positional indexing is all messed.\n",
    "# let's fix that before we sample.\n",
    "\n",
    "ipeds = ipeds.reset_index().drop(columns = \"index\")\n",
    "\n",
    "# remove the NaNs by putting in the column means for floats, and most common for \n",
    "# categories\n",
    "\n",
    "ipeds_f_means = ipeds.select_dtypes(include=[\"float64\", \"float\"]).apply(np.nanmean, axis=0)\n",
    "ipeds_c_means = ipeds.select_dtypes(include=[\"category\"]).apply(scipy.stats.mode, axis=0, nan_policy='omit')\n",
    "ipeds_c_means = ipeds_c_means.iloc[0]\n",
    "\n",
    "ipeds_f = ipeds.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "for col in ipeds_f.select_dtypes(include=[\"category\"]):\n",
    "    if ipeds_f[col].isnull().sum() > 0:\n",
    "        ipeds_f[col] = ipeds_f[col].fillna(ipeds_c_means[col][0])\n",
    "\n",
    "for col in ipeds_f.select_dtypes(include=[\"float64\"]):\n",
    "    if ipeds_f[col].isnull().sum() > 0:\n",
    "        ipeds_f[col] = ipeds_f[col].fillna(ipeds_f_means[col])\n",
    "            \n",
    "# get the training and test sets, and drop the response variable\n",
    "\n",
    "ipeds_training = ipeds_f.sample(frac=0.9, random_state=55, axis=0)\n",
    "test_index = [x for x in list(ipeds_f.index) if x not in list(ipeds_training.index)]\n",
    "ipeds_test = ipeds_f.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quin/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "regressortree.fit(ipeds_training.drop(columns = responsevar), ipeds_training[responsevar])\n",
    "\n",
    "importance = pd.DataFrame(regressortree.feature_importances_)\n",
    "importance = importance.rename(columns = {0:'Importance'}).sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.066557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.042426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.041399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.021379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.014049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.013736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.013627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.013155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.012902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Importance\n",
       "124    0.066557\n",
       "35     0.042426\n",
       "16     0.041399\n",
       "209    0.026000\n",
       "192    0.021379\n",
       "203    0.014049\n",
       "328    0.013736\n",
       "200    0.013627\n",
       "153    0.013155\n",
       "207    0.012902"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HRCHG3', 'LEVEL3', 'C15UGPRF', 'ANYAIDP', 'EFDEEX5 _3', 'UPGRNTP'], dtype='object')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipeds_f.columns[importance.index[0:6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features that contribute to the retention rate, according to this one run of a random forest regression, are as follows:\n",
    "\n",
    "* **HRCHG3** - Out-of-state per credit hour charge for part-time undergraduates\n",
    "* **LEVEL3** - Whether the institution offers an associate's degree\n",
    "* **C15UGPRF** - The Carnegie Undergraduate Profile Classification\n",
    "* **ANYAIDP** - Percent of full-time first-time undergraduates awarded any financial aid\n",
    "* **EFDEEX5_3** - Students enrolled exclusively in distance education courses and location of student unknown/not reported; specifically those who are seeking degrees\n",
    "* **UPGRNTP** - Percent of undergraduate students awarded Pell grants. (Pell grants are for lower-income students.)\n",
    "\n",
    "Before we test the data, some interesting points:\n",
    "\n",
    "* Nothing from the American Community Survey has appeared in the top ten, here.\n",
    "* The feature importances reported aren't terribly high. I don't know whether the low power is because\n",
    "of the large number of other features, or because none of these are particularly effective.\n",
    "* I also don't know enough about scikit-learn *yet* to be able to get *p*-values from these features, which would be a bit more useful. I'll do some research.\n",
    "\n",
    "Secondly, some ways forward.\n",
    "\n",
    "* This is a single example of a single run of a random forest regression algorithm. In order to make sure this wasn't a fluke, we'll need to run it quite a few times... and perhaps try a couple of other\n",
    "appropriate algorithms, too. However, the main work of picking, loading and cleaning the data is done.\n",
    "* As per the literature review, slicing these into college types (private, public / 2-year, 4-year) yielded some more useful results. This is borne out by some of the features that appeared as the most important -- degree type, student type. Can we filter down and get some specific recommendations?\n",
    "* Some colleges are at 0% first-year retention, and some others are at 100%. Why? Will removing these make the model more reliable, or not?\n",
    "* Should we ditch using the ACS entirely? I noticed it didn't contain all of the counties. Should we use 5-year data instead?\n",
    "* I would like those red messages to go away.\n",
    "\n",
    "Finally, let's test if this model is sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 16.1444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ipeds_test[\"Prediction\"] = regressortree.predict(ipeds_test.drop(columns = responsevar))\n",
    "ipeds_test[\"squareerror\"] = (ipeds_test[responsevar] - ipeds_test[\"Prediction\"])**2\n",
    "print(\"Root mean squared error: \"+str(round(float(np.sqrt(ipeds_test[\"squareerror\"].mean())), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retention rates are measured in percentages, and a RMSE of +/- 8 percentage points could be worse, I suppose. Let's see if we can get this more accurate with further testing and model refinement. And fewer red warning messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~ Quin Parker\n",
    "\n",
    "Note: Help was provided by *stackoverflow.com* and *datasciencerosettastone.com*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
