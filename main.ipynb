{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention rates for US Universities\n",
    "\n",
    "This project is being built from the ground up to be customizable and reproducible.\n",
    "\n",
    "After importing the libraries we need, we're going to load up a table of instructions containing which\n",
    "of the several thousand attributes we're going to select from the IPEDS database for data mining. This is a lot, so it seems more reasonable to externally store the data list we'll be drawing from.\n",
    "\n",
    "Many of these, such as website addresses or mission statements, aren't going to be terribly useful. Some others we'll need to exclude as it is too closely related to retention rate, and we risk overfitting or circular logic (\"hey, here's how to raise your retention rate--have more of them graduate!\")\n",
    "\n",
    "At the root, each entry in the JSON file denotes a separate table. Included alongside the table name are instructions on whether all the table should be imported as default or not, which attributes are continuous, which are discrete, and  which are strings, and whether multiple records exist for each primary key. This is all derived from the associated documentation that comes with IPEDS. \n",
    "\n",
    "We're going to load in each table (obviously checking that it doesn't exist first), and extract the correct tables from it. \n",
    "\n",
    "(Should you want to change what we're measuring, you can change the predictive variable within the JSON file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import json\n",
    "import wget\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from zipfile import ZipFile as zf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we're going to set some variables for how (and where) we process the data.\n",
    "\n",
    "* *MAXIMUM_NAN* : the ratio of NaNs at which we remove the column entirely.\n",
    "* *MAXIMUM_COR* : the maximum Pearson correlation that a column can have with another before it's removed.\n",
    "* *DATA_PATH* : where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXIMUM_NAN = 0.80\n",
    "MAXIMUM_COR = 0.85\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading in the IPEDS data\n",
    "\n",
    "Next come the functions for reading in the data from files/ZIP archives, corraling them all to one row per unique key, and making sure they're the right data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported HD2016: 27 columns total, 0.4MB\n",
      "Imported IC2016: 132 columns total, 1.67MB\n",
      "Imported IC2016_AY: 252 columns total, 8.56MB\n",
      "Imported ADM2016: 290 columns total, 10.74MB\n",
      "Imported EFFY2016: 380 columns total, 15.91MB\n",
      "Imported EF2016A: 650 columns total, 31.4MB\n",
      "Imported EF2016B: 776 columns total, 38.63MB\n",
      "Imported EF2016C: 906 columns total, 46.09MB\n",
      "Imported EF2016D: 917 columns total, 46.72MB\n",
      "Imported EF2016A_DIST: 962 columns total, 49.3MB\n",
      "Imported SFA1516: 1285 columns total, 67.83MB\n",
      "Imported SFAV1516: 1303 columns total, 68.87MB\n",
      "Imported F1516_F1A: 1413 columns total, 75.13MB\n",
      "Imported F1516_F2: 1553 columns total, 83.11MB\n",
      "Imported F1516_F3: 1632 columns total, 87.53MB\n",
      "Imported EAP2016: 3990 columns total, 222.84MB\n",
      "Imported SAL2016_IS: 4123 columns total, 230.47MB\n",
      "Imported SAL2016_NIS: 4151 columns total, 232.08MB\n",
      "Imported S2016_OC: 4833 columns total, 271.21MB\n",
      "Imported S2016_SIS: 4910 columns total, 275.63MB\n",
      "Imported S2016_NH: 4941 columns total, 277.41MB\n",
      "Imported AL2016: 4971 columns total, 279.13MB\n"
     ]
    }
   ],
   "source": [
    "def convert(tdf, columnlist, totype):\n",
    "\n",
    "# this changes the datatype in each column. where columns have been unstacked, this is supposed\n",
    "# to change all instances of each unstacked column to that datatype\n",
    "\n",
    "    for col in columnlist:\n",
    "        r = re.compile(\"(^\"+col+\"*)|(_\"+col+\")\")\n",
    "        # column name must either be first or have a _ before it, depending on whether we're\n",
    "        # putting the table names next to it to make it unique\n",
    "        for c in filter(r.match, list(tdf)): # filter all the column names through this regex\n",
    "            tdf[c] = tdf[c].astype(totype)\n",
    "\n",
    "def import_data(filename):\n",
    "    with open(filename) as file:\n",
    "        instructions = json.load(file)\n",
    "        \n",
    "# does the data directory exist? if no, create it.\n",
    "    \n",
    "    if os.path.isdir(DATA_PATH) is False:\n",
    "        try:  \n",
    "            os.mkdir(DATA_PATH)\n",
    "        except OSError:  \n",
    "            print (\"Could not create data folder.\")\n",
    "            raise\n",
    "            \n",
    "# start with an empty dataframe to fill, and get the primary key.\n",
    "# the primary key should be in the first file in the data list\n",
    "# also figure out if we are adding the tablename to the attributes to make them unique\n",
    "\n",
    "    ourdata = pd.DataFrame()\n",
    "    pk = instructions[\"primarykey\"]\n",
    "    unique_headers = instructions[\"uniqueheaders\"]\n",
    "\n",
    "# loop through each table. \n",
    "\n",
    "    for table in instructions[\"tables\"]:\n",
    "        \n",
    "# first, check if the file in the table has been downloaded. if not, download it from\n",
    "# the path given and throw an error if something is wrong.\n",
    "\n",
    "        filename = table[\"name\"]+instructions[\"format\"]\n",
    "        filelocation = DATA_PATH+\"/\"+filename\n",
    "        csvfile = DATA_PATH+\"/\"+table[\"name\"]+\".csv\"\n",
    "\n",
    "        if os.path.exists(filelocation) is False:\n",
    "            try:\n",
    "                wget.download(instructions[\"url\"]+filename, filelocation)\n",
    "            except Exception as e:\n",
    "                print(\"Problem downloading and saving\", table[\"name\"], \":\", e)\n",
    "                raise\n",
    "\n",
    "# next, if they are zip files, unzip them\n",
    "\n",
    "        if instructions[\"format\"] == \".zip\":\n",
    "            if os.path.exists(str(csvfile).lower()) is False:\n",
    "                print(\"Unzipping \"+table[\"name\"])\n",
    "                with zf(filelocation,\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "# load each CSV file into a temporary data frame, tdf\n",
    "\n",
    "        tdf = pd.read_csv(str(csvfile).lower(), encoding = \"ISO-8859-1\")\n",
    "\n",
    "# filter the data according to any values needed\n",
    "\n",
    "        if \"filter\" in table:\n",
    "            for filterinfo in table[\"filter\"]:\n",
    "                tdf = tdf.loc[tdf[filterinfo[0]] == filterinfo[1]]\n",
    "\n",
    "# then, depending on the instructions in the JSON file, include all the headers, or include a selection.\n",
    "\n",
    "        if table[\"includeall\"]:\n",
    "\n",
    "# include the whole list, excluding the specific tables (might be an empty list)\n",
    "# and add the primary key to select\n",
    "\n",
    "            to_include = list(tdf)\n",
    "            to_exclude = table[\"exclude\"]\n",
    "            headers = [x for x in to_include if x not in to_exclude]\n",
    "            headers.append(pk)\n",
    "\n",
    "# otherwise, stick these three lists together plus the primary key\n",
    "\n",
    "        else:\n",
    "            headers = [pk, *table[\"strings\"], *table[\"discrete\"], *table[\"continuous\"]]\n",
    "\n",
    "        selected_headers = [x for x in tdf.columns if x in headers]\n",
    "\n",
    "# columns that begin with an X should be removed, at least for now, because they don't describe\n",
    "# anything other than how the data was collected\n",
    "    \n",
    "        selected_headers = [x for x in selected_headers if x[:1] is not \"X\"]\n",
    "\n",
    "# ok, so do we have a primary key? if not, stop right there\n",
    "\n",
    "        if pk not in tdf.columns:\n",
    "            raise KeyError(\"Primary key \"+pk+\" not found in \"+table[\"name\"])\n",
    "\n",
    "# now we can select the headers we want.\n",
    "\n",
    "        tdf = tdf[selected_headers]\n",
    "    \n",
    "# the code below adds the table name to the headers now, to prevent issues with duplication\n",
    "# to everythng other than the key\n",
    "\n",
    "        if unique_headers:\n",
    "            tdf.rename(columns = lambda x: pk if x == pk else table[\"name\"]+\"_\"+x, inplace = True)\n",
    "\n",
    "# next we must check in the JSON instructions if this table contains multiple rows for each\n",
    "# unique ID. if so, we need to put them all on the same row. to do this, we change them to strings\n",
    "# then read them into a multiple index, unstack, and then join the column names.\n",
    "\n",
    "        if \"multi\" in table:\n",
    "\n",
    "            if unique_headers:\n",
    "                multi = [table[\"name\"]+\"_\"+x for x in table[\"multi\"]]\n",
    "            else:\n",
    "                multi = table[\"multi\"]\n",
    "            tdf[multi] = tdf[multi].astype(str)\n",
    "            tdf = tdf.set_index([pk, *multi])\n",
    "            tdf = tdf.unstack(multi)               # need to specify ALL the levels\n",
    "            tdf.columns = ['_'.join(col) for col in tdf.columns.values]\n",
    "            tdf = tdf.reset_index(level=pk)\n",
    "\n",
    "# any '.', '. ', '(X)' data should be NaN\n",
    "\n",
    "        tdf = tdf.replace(r\"\\. ?\", np.nan, regex=True)\n",
    "\n",
    "# one important thing to do here is to set the right data types--strings or discrete data.\n",
    "# most of the data in IPEDS and ACS are continuous, but there are a couple of strings and a \n",
    "# handful of discrete data.\n",
    "\n",
    "        if \"defaulttype\" in table:\n",
    "            tdfheds = list(tdf) # get list of headers\n",
    "            hedstoremove = [pk, *table[\"strings\"], *table[\"discrete\"], *table[\"continuous\"]]\n",
    "            tdfheds = [x for x in tdfheds if x not in hedstoremove]\n",
    "            if table[\"defaulttype\"] == \"discrete\":\n",
    "                convert(tdf, tdfheds, \"category\")  \n",
    "            elif table[\"defaulttype\"] == \"string\":\n",
    "                convert(tdf, tdfheds, \"str\")\n",
    "            else:\n",
    "                convert(tdf, tdfheds, \"float\")\n",
    "                # only way to store NaNs\n",
    "\n",
    "        convert(tdf, table[\"strings\"], \"str\")\n",
    "        convert(tdf, table[\"discrete\"], \"category\")\n",
    "        convert(tdf, table[\"continuous\"], \"float\")\n",
    "        \n",
    "        if pk in tdf:\n",
    "            tdf[pk] = tdf[pk].astype('str') # it is in the index if the data is unstacked\n",
    "\n",
    "# if it's the first time around the loop, take the first set of data.\n",
    "\n",
    "        if ourdata.empty:\n",
    "            ourdata = tdf\n",
    "\n",
    "# if not, then we need to join on the primary key\n",
    "\n",
    "        else:\n",
    "            ourdata = ourdata.merge(tdf,on=pk,how=\"left\")\n",
    "\n",
    "        print(\"Imported \"+ table[\"name\"]+ \": \"+str(len(ourdata.columns))+\" columns total, \"\n",
    "              + str(round(float(ourdata.memory_usage().sum() / 1048576), 2)) + \"MB\")\n",
    "    \n",
    "    if \"predictive\" in table:\n",
    "        return ourdata, table[\"predictive\"]\n",
    "    else\n",
    "        return ourdata, None\n",
    "\n",
    "ipeds, predictive = import_data(\"ipeds-instructions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the two-letter state codes and the county names. We now need to convert them to County Name, State in order to make the join with the American Community Survey data. We'll create a new column and drop the old county name column.\n",
    "\n",
    "As you can see from the selection below, there's a one-to-many relationship between the counties and the educational institutions. And, of course, many counties won't have institutions in at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000       Dupage County, Illinois\n",
      "1001         Cook County, Illinois\n",
      "1002         Cook County, Illinois\n",
      "1003         Cook County, Illinois\n",
      "1004         Cook County, Illinois\n",
      "1005         Cook County, Illinois\n",
      "1006         Cook County, Illinois\n",
      "1007         Cook County, Illinois\n",
      "1008         Cook County, Illinois\n",
      "1009         Cook County, Illinois\n",
      "1010         Cook County, Illinois\n",
      "1011         Cook County, Illinois\n",
      "1012         Cook County, Illinois\n",
      "1013         Cook County, Illinois\n",
      "1014         Cook County, Illinois\n",
      "1015         Cook County, Illinois\n",
      "1016         Cook County, Illinois\n",
      "1017    Vermilion County, Illinois\n",
      "1018    Vermilion County, Illinois\n",
      "1019         Cook County, Illinois\n",
      "1020       Dupage County, Illinois\n",
      "1021      Mchenry County, Illinois\n",
      "1022       Dupage County, Illinois\n",
      "1023         Cook County, Illinois\n",
      "1024        Coles County, Illinois\n",
      "Name: County_Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with open(\"states_hash.json\") as file:\n",
    "    states = json.load(file)\n",
    "if \"County_Name\" not in ipeds:\n",
    "    ipeds[\"County_Name\"] = ipeds[\"COUNTYNM\"]+\", \"+ipeds[\"STABBR\"].map(states)\n",
    "    ipeds = ipeds.drop(columns=\"COUNTYNM\")\n",
    "print(ipeds[\"County_Name\"][1000:1025])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Preparing the data for analysis\n",
    "\n",
    "This is, clearly, a horrific amount of data. Let's remove surplus columns before putting them under analysis. First, find out how many NaNs there are for each column and remove those above the threshold. (We should do this before figuring out correlations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.96% of columns removed for missing too much data\n"
     ]
    }
   ],
   "source": [
    "original_length = len(ipeds.columns)\n",
    "\n",
    "nanlist = ipeds.isna().sum(axis=0) / len(ipeds)\n",
    "nans = nanlist.loc[nanlist >= MAXIMUM_NAN]\n",
    "print (str(round(float(len(nans) / len(ipeds)),4) * 100) + \n",
    "       \"% of columns removed for missing too much data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to find out which datasets are too similar to one another, and remove them. \n",
    "The most basic way to do this is to do a Pearson correlation between all of the attributes, and\n",
    "then remove one of each pair, or all-but-one of each collection.\n",
    "\n",
    "As there are around 4,000 attributes, this will take a little while to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pearson correlation between each column. \n",
    "\n",
    "cmatrix = ipeds.corr(method=\"pearson\").abs()\n",
    "\n",
    "# change the matrix to a dataframe, and put the indices in the first two columns\n",
    "\n",
    "c = cmatrix.unstack().to_frame().reset_index()\n",
    "c.columns = [\"A\", \"B\", \"Correlation\"]\n",
    "\n",
    "# remove NaNs\n",
    "c = c.dropna()\n",
    "\n",
    "# remove pairs (everything correlates with itself)\n",
    "c = c.loc[c[\"A\"] != c[\"B\"]]\n",
    "\n",
    "# and remove everything that is less than the maximum correlation.\n",
    "c = c.loc[c[\"Correlation\"] >= MAXIMUM_COR]\n",
    "\n",
    "print (c.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of attributes: 4971\n",
      "Reduced number of attributes: 732\n",
      "14.729999999999999% of original.\n"
     ]
    }
   ],
   "source": [
    "# we now have a list of pairs of variables that correlate with each other.\n",
    "# now we need to extract which variables to remove from our main dataframe.\n",
    "\n",
    "# first, let's sort the two columns into alphabetical order so that the first always lies behind\n",
    "# the second. we do this by applying a lambda across each row and broadcasting the result back\n",
    "# into the dataframe. \n",
    "\n",
    "c = c.apply(lambda row: [row[\"B\"], row[\"A\"], row[\"Correlation\"]] \n",
    "                         if row[\"A\"] > row[\"B\"] else [row[\"A\"], row[\"B\"], row[\"Correlation\"]],\n",
    "                         axis=1, result_type='broadcast')\n",
    "\n",
    "# we then drop all the duplicates. this should remove probably half or so of the rows.\n",
    "\n",
    "c = c.drop_duplicates()\n",
    "\n",
    "# if we then just take the first column, and dedupe that, we'll get a list of columns we can remove.\n",
    "# if there are more than 2 columns that correlate with one another, one column will always\n",
    "# only appear in the 'B' list. this will be the one that goes forward to the next stage.\n",
    "\n",
    "cols_to_remove = c[\"A\"].drop_duplicates()\n",
    "\n",
    "ipeds = ipeds.drop(columns = cols_to_remove)\n",
    "\n",
    "print (\"Original number of attributes: \"+str(original_length))\n",
    "print (\"Reduced number of attributes: \"+str(len(ipeds.columns)))\n",
    "print (str(round(float(len(ipeds.columns) / original_length),4) * 100) + \"% of original.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
