{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention rates for US Universities\n",
    "\n",
    "This project is being built from the ground up to be customizable and reproducible.\n",
    "\n",
    "After importing the libraries we need, we're going to load up a table of instructions containing which\n",
    "of the 2,500+ attributes we're going to select from the IPEDS database for data mining. This is a lot,\n",
    "so it seems more reasonable to externally store the data list we'll be drawing from.\n",
    "\n",
    "Many of these, such as website addresses or mission statements, aren't going to be terribly useful. Some others we'll need to exclude as it is too closely related to retention rate, and we risk overfitting or circular logic (\"hey, here's how to raise your retention rate--have more of them graduate!\")\n",
    "\n",
    "At the root, each entry in the JSON file denotes a separate table. Included alongside the table name are instructions on whether all the table should be imported as default or not, which attributes are continuous, which are discrete, and  which are strings, and whether multiple records exist for each primary key. This is all derived from the associated documentation that comes with IPEDS. \n",
    "\n",
    "We're going to load in each table (obviously checking that it doesn't exist first), and extract the correct tables from it. \n",
    "\n",
    "(Should you want to change what we're measuring, you can change the predictive variable within the JSON file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported HD2016: 26 columns total, 1.49MB\n",
      "Imported IC2016: 131 columns total, 7.57MB\n",
      "Imported IC2016_AY: 251 columns total, 14.46MB\n",
      "Imported ADM2016: 289 columns total, 16.64MB\n",
      "Imported EFFY2016: 379 columns total, 21.8MB\n",
      "Imported EF2016A: 649 columns total, 37.3MB\n",
      "Imported EF2016B: 775 columns total, 44.53MB\n",
      "Imported EF2016C: 905 columns total, 51.99MB\n",
      "Imported EF2016D: 916 columns total, 52.62MB\n",
      "Imported EF2016A_DIST: 961 columns total, 55.2MB\n",
      "Imported SFA1516: 1284 columns total, 73.73MB\n",
      "Imported SFAV1516: 1302 columns total, 74.77MB\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'includeall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a6fc260177a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m               + str(round(float(ourdata.memory_usage().sum() / 1048576), 2)) + \"MB\")\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mimport_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ipeds-instructions.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-a6fc260177a6>\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# then, depending on the instructions in the JSON file, include all the headers, or include a selection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"includeall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# include the whole list, excluding the specific tables (might be an empty list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'includeall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import json\n",
    "import wget\n",
    "import sys\n",
    "import os\n",
    "from zipfile import ZipFile as zf\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "def import_data(filename):\n",
    "    with open(filename) as file:\n",
    "        instructions = json.load(file)\n",
    "        \n",
    "# does the data directory exist? if no, create it.\n",
    "    \n",
    "    if os.path.isdir(DATA_PATH) is False:\n",
    "        try:  \n",
    "            os.mkdir(DATA_PATH)\n",
    "        except OSError:  \n",
    "            print (\"Could not create data folder.\")\n",
    "            raise\n",
    "            \n",
    "# start with an empty dataframe to fill, and get the primary key.\n",
    "# the primary key should be in the first file in the data list\n",
    "\n",
    "    ourdata = pd.DataFrame()\n",
    "    pk = instructions[\"primarykey\"]\n",
    "\n",
    "# loop through each table. \n",
    "\n",
    "    for table in instructions[\"tables\"]:\n",
    "        \n",
    "# first, check if the file in the table has been downloaded. if not, download it from\n",
    "# the path given and throw an error if something is wrong.\n",
    "\n",
    "        filename = table[\"name\"]+instructions[\"format\"]\n",
    "        filelocation = DATA_PATH+\"/\"+filename\n",
    "        csvfile = DATA_PATH+\"/\"+table[\"name\"]+\".csv\"\n",
    "\n",
    "        if os.path.exists(filelocation) is False:\n",
    "            try:\n",
    "                wget.download(instructions[\"url\"]+filename, filelocation)\n",
    "            except Exception as e:\n",
    "                print(\"Problem downloading and saving\", table[\"name\"], \":\", e)\n",
    "                raise\n",
    "\n",
    "# next, if they are zip files, unzip them\n",
    "\n",
    "        if instructions[\"format\"] == \".zip\":\n",
    "            if os.path.exists(str(csvfile).lower()) is False:\n",
    "                print(\"Unzipping \"+table[\"name\"])\n",
    "                with zf(filelocation,\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "# load each CSV file into a temporary data frame, tdf\n",
    "\n",
    "        tdf = pd.read_csv(str(csvfile).lower(), encoding = \"ISO-8859-1\")\n",
    "\n",
    "# filter the data according to any values needed\n",
    "\n",
    "        if \"filter\" in table:\n",
    "            for filterinfo in table[\"filter\"]:\n",
    "                tdf = tdf.loc[tdf[filterinfo[0]] == filterinfo[1]]\n",
    "\n",
    "# then, depending on the instructions in the JSON file, include all the headers, or include a selection.\n",
    "\n",
    "        if table[\"includeall\"]:\n",
    "\n",
    "# include the whole list, excluding the specific tables (might be an empty list)\n",
    "# and add the primary key to select\n",
    "\n",
    "            to_include = list(tdf)\n",
    "            to_exclude = table[\"exclude\"]\n",
    "            headers = [x for x in to_include if x not in to_exclude]\n",
    "            headers.append(pk)\n",
    "\n",
    "# otherwise, stick these three lists together plus the primary key\n",
    "\n",
    "        else:\n",
    "            headers = [pk, *table[\"strings\"], *table[\"discrete\"], *table[\"continuous\"]]\n",
    "\n",
    "        selected_headers = [x for x in tdf.columns if x in headers]\n",
    "\n",
    "# columns that begin with an X should be removed, at least for now, because they don't describe\n",
    "# anything other than how the data was collected\n",
    "    \n",
    "        selected_headers = [x for x in selected_headers if x[:1] is not \"X\"]\n",
    "\n",
    "# ok, so do we have a primary key? if not, stop right there\n",
    "\n",
    "        if pk not in tdf.columns:\n",
    "            raise KeyError(\"Primary key \"+pk+\" not found in \"+table[\"name\"])\n",
    "\n",
    "# now we can select the headers we want.\n",
    "\n",
    "        tdf = tdf[selected_headers]\n",
    "    \n",
    "# the code below adds the table name to the headers now, to prevent issues with duplication\n",
    "# to everythng other than the key\n",
    "\n",
    "#        tdf.rename(columns = lambda x: pk if x == pk else table[\"name\"]+\"_\"+x, inplace = True)\n",
    "\n",
    "# next we must check in the JSON instructions if this table contains multiple rows for each\n",
    "# unique ID. if so, we need to put them all on the same row. to do this, we change them to strings\n",
    "# then read them into a multiple index, unstack, and then join the column names.\n",
    "\n",
    "        if \"multi\" in table:\n",
    "\n",
    "            multi = table[\"multi\"]\n",
    "# to use if we are adding table headers to column names\n",
    "#\n",
    "#            multi = [table[\"name\"]+\"_\"+x for x in table[\"multi\"]]\n",
    "            tdf[multi] = tdf[multi].astype(str)\n",
    "            tdf = tdf.set_index([pk, *multi])\n",
    "            tdf = tdf.unstack(multi)               # need to specify ALL the levels\n",
    "            tdf.columns = ['_'.join(col) for col in tdf.columns.values]\n",
    "\n",
    "# if it's the first time around the loop, take the first set of data.\n",
    "\n",
    "        if ourdata.empty:\n",
    "            ourdata = tdf\n",
    "\n",
    "# if not, then we need to join on the primary key\n",
    "\n",
    "        else:\n",
    "            ourdata = ourdata.merge(tdf,on=pk,how=\"left\")\n",
    "\n",
    "        print(\"Imported \"+ table[\"name\"]+ \": \"+str(len(ourdata.columns))+\" columns total, \"\n",
    "              + str(round(float(ourdata.memory_usage().sum() / 1048576), 2)) + \"MB\")\n",
    "              \n",
    "import_data(\"ipeds-instructions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
