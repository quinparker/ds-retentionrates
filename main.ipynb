{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention rates for US Universities\n",
    "\n",
    "This project is being built from the ground up to be customizable and reproducible.\n",
    "\n",
    "After importing the libraries we need, we're going to load up a table of instructions containing which\n",
    "of the 2,500+ attributes we're going to select from the IPEDS database for data mining. This is a lot,\n",
    "so it seems more reasonable to externally store the data list we'll be drawing from.\n",
    "\n",
    "Many of these, such as website addresses or mission statements, aren't going to be terribly useful. Some others we'll need to exclude as it is too closely related to retention rate, and we risk overfitting or circular logic (\"hey, here's how to raise your retention rate--have more of them graduate!\")\n",
    "\n",
    "At the root, each entry in the JSON file denotes a separate table. Included alongside the table name are instructions on whether all the table should be imported as default or not, which attributes are continuous, which are discrete, and  which are strings, and whether multiple records exist for each primary key. This is all derived from the associated documentation that comes with IPEDS. \n",
    "\n",
    "We're going to load in each table (obviously checking that it doesn't exist first), and extract the correct tables from it. \n",
    "\n",
    "(Should you want to change what we're measuring, you can change the predictive variable within the JSON file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'includeall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-eb70610b2460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# TODO: FIGURE OUT HOW TO JOIN THE OLD AND NEW DATAFRAMES ON THE KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mimport_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ipeds-instructions.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-eb70610b2460>\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# then, depending on the instructions in the JSON file, include all the headers, or include a selection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"includeall\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# include the whole list, excluding the specific tables (might be an empty list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'includeall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import json\n",
    "import wget\n",
    "import sys\n",
    "import os\n",
    "from zipfile import ZipFile as zf\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "def import_data(filename):\n",
    "    with open(filename) as file:\n",
    "        instructions = json.load(file)\n",
    "        \n",
    "# does the data directory exist? if no, create it.\n",
    "    \n",
    "    if os.path.isdir(DATA_PATH) is False:\n",
    "        try:  \n",
    "            os.mkdir(DATA_PATH)\n",
    "        except OSError:  \n",
    "            print (\"Could not create data folder.\")\n",
    "            raise\n",
    "            \n",
    "# start with an empty dataframe to fill, and get the primary key.\n",
    "# the primary key should be in the first file in the data list\n",
    "\n",
    "    ourdata = pd.DataFrame()\n",
    "    pk = instructions[\"primarykey\"]\n",
    "\n",
    "# loop through each table. \n",
    "\n",
    "    for table in instructions[\"tables\"]:\n",
    "        \n",
    "# first, check if the file in the table has been downloaded. if not, download it from\n",
    "# the path given and throw an error if something is wrong.\n",
    "\n",
    "        filename = table[\"name\"]+instructions[\"format\"]\n",
    "        filelocation = DATA_PATH+\"/\"+filename\n",
    "        csvfile = DATA_PATH+\"/\"+table[\"name\"]+\".csv\"\n",
    "\n",
    "        if os.path.exists(filelocation) is False:\n",
    "            try:\n",
    "                wget.download(instructions[\"url\"]+filename, filelocation)\n",
    "            except Exception as e:\n",
    "                print(\"Problem downloading and saving\", table[\"name\"], \":\", e)\n",
    "                raise\n",
    "\n",
    "# next, if they are zip files, unzip them\n",
    "\n",
    "        if instructions[\"format\"] == \".zip\":\n",
    "            if os.path.exists(str(csvfile).lower()) is False:\n",
    "                print(\"Unzipping \"+table[\"name\"])\n",
    "                with zf(filelocation,\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "# load each CSV file into a temporary data frame\n",
    "\n",
    "        transitional_df = pd.read_csv(str(csvfile).lower(), encoding = \"ISO-8859-1\")\n",
    "\n",
    "# then, depending on the instructions in the JSON file, include all the headers, or include a selection.\n",
    "\n",
    "        if table[\"includeall\"]:\n",
    "\n",
    "# include the whole list, excluding the specific tables (might be an empty list)\n",
    "# and add the primary key to select\n",
    "\n",
    "            to_include = list(transitional_df)\n",
    "            to_exclude = table[\"exclude\"]\n",
    "            headers = [x for x in to_include if x not in to_exclude]\n",
    "            headers.append(pk)\n",
    "\n",
    "# stick these three lists together plus the primary key\n",
    "\n",
    "        else:\n",
    "            headers = [pk, *table[\"strings\"], *table[\"discrete\"], *table[\"continuous\"]]\n",
    "\n",
    "        selected_headers = [x for x in transitional_df.columns if x in headers]\n",
    "\n",
    "# if it's the first time around, take the first set of data, checking for the primary key existence\n",
    "\n",
    "        if ourdata.empty:\n",
    "            if pk not in transitional_df.columns:\n",
    "                raise KeyError(\"Primary key \"+pk+\" not found in first file mentioned\")\n",
    "            ourdata = transitional_df[selected_headers]\n",
    "\n",
    "# if not, then we need to join on the primary key\n",
    "\n",
    "# TODO: FIGURE OUT HOW TO JOIN THE OLD AND NEW DATAFRAMES ON THE KEY\n",
    "            \n",
    "import_data(\"ipeds-instructions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
